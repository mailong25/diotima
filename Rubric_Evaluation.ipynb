{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ac6dec",
   "metadata": {},
   "source": [
    "# Metrics:\n",
    " - Embedding similarity\n",
    " - Rouge-L\n",
    " - LLM score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a187b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.api_clients import get_embedding\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "\n",
    "def embedding_similarity(text1, text2):\n",
    "    embeddings1 = np.array(get_embedding(text1))\n",
    "    embeddings2 = np.array(get_embedding(text2))\n",
    "\n",
    "    cosine_sim = np.dot(embeddings1, embeddings2) / (\n",
    "        np.linalg.norm(embeddings1) * np.linalg.norm(embeddings2)\n",
    "    )\n",
    "    return float(cosine_sim)\n",
    "\n",
    "def rouge_l_score(text1, text2):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    score = scorer.score(text1, text2)\n",
    "    return float(score['rougeL'].fmeasure)\n",
    "\n",
    "def llm_score(rubric1, rubric2, rubric_level, question):\n",
    "    from utils.prompt_processing import run_prompt\n",
    "\n",
    "    prompt_vars = {\n",
    "        \"rubric1\": rubric1,\n",
    "        \"rubric2\": rubric2,\n",
    "        \"rubric_level\": rubric_level,\n",
    "        \"question\": question\n",
    "    }\n",
    "\n",
    "    resp = run_prompt(\n",
    "        template_path=\"prompts/evaluation.yaml\",\n",
    "        template_key=\"rubric_evaluation\",\n",
    "        template_vars=prompt_vars,\n",
    "        model=\"openai/gpt-5-mini\",\n",
    "        base_temperature=0.0,\n",
    "    )\n",
    "\n",
    "    score_match = re.search(r\"Final\\s*score:\\s*([0-9.]+)\", resp, re.IGNORECASE)\n",
    "    if score_match:\n",
    "        return float(score_match.group(1)) / 5.0  # Normalize to 0-1 scale\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def compute_score(gen_rubric, ref_rubric, question=\"\"):\n",
    "    \"\"\"\n",
    "    Compare generated rubric vs reference rubric across all levels.\n",
    "    Returns:\n",
    "        dict with per-level and overall average scores.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    embedding_scores = []\n",
    "    rouge_scores = []\n",
    "    llm_scores = []\n",
    "\n",
    "    for level in gen_rubric:\n",
    "        gen_text = gen_rubric[level]\n",
    "        ref_text = ref_rubric.get(level, \"\")\n",
    "\n",
    "        if not ref_text:\n",
    "            # Skip levels missing from reference\n",
    "            continue\n",
    "\n",
    "        emb_sim = embedding_similarity(gen_text, ref_text)\n",
    "        rouge_sim = rouge_l_score(gen_text, ref_text)\n",
    "        llm_sim = llm_score(ref_text, gen_text, level, question)\n",
    "\n",
    "        results[level] = {\n",
    "            \"embedding_similarity\": emb_sim,\n",
    "            \"rougeL\": rouge_sim,\n",
    "            \"llm_score\": llm_sim,\n",
    "        }\n",
    "\n",
    "        embedding_scores.append(emb_sim)\n",
    "        rouge_scores.append(rouge_sim)\n",
    "        if llm_sim is not None:\n",
    "            llm_scores.append(llm_sim)\n",
    "\n",
    "    # Compute overall averages\n",
    "    results[\"average\"] = {\n",
    "        \"embedding_similarity\": np.mean(embedding_scores) if embedding_scores else 0.0,\n",
    "        \"rougeL\": np.mean(rouge_scores) if rouge_scores else 0.0,\n",
    "        \"llm_score\": np.mean(llm_scores) if llm_scores else 0.0,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92c9d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Describe the role of mitochondria in cellular respiration.\"\n",
    "rubric = {\n",
    "    \"Emerging\": \"Shows minimal understanding; major misconceptions or irrelevant response.\",\n",
    "    \"Developing\": \"Demonstrates basic understanding; contains some errors or incomplete explanation.\",\n",
    "    \"Proficient\": \"Shows good understanding; mostly accurate with minor omissions or inaccuracies.\",\n",
    "    \"Advanced\": \"Demonstrates excellent understanding with a complete and accurate explanation.\"\n",
    "}\n",
    "rubric2 = {\n",
    "    \"Emerging\": \"Shows limited understanding with several misconceptions or irrelevant statements.\",\n",
    "    \"Developing\": \"Demonstrates some understanding; explanation is basic and may include minor errors or missing details.\",\n",
    "    \"Proficient\": \"Shows clear understanding; mostly accurate with only a few small omissions.\",\n",
    "    \"Advanced\": \"Demonstrates strong understanding with a thorough and precise explanation of the mitochondria's role in respiration.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3314aebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Emerging': {'embedding_similarity': 0.846237032925695,\n",
       "  'rougeL': 0.5882352941176471,\n",
       "  'llm_score': 0.8},\n",
       " 'Developing': {'embedding_similarity': 0.9184320083035457,\n",
       "  'rougeL': 0.34782608695652173,\n",
       "  'llm_score': 1.0},\n",
       " 'Proficient': {'embedding_similarity': 0.8920768122003521,\n",
       "  'rougeL': 0.5714285714285713,\n",
       "  'llm_score': 0.8},\n",
       " 'Advanced': {'embedding_similarity': 0.6308014881395728,\n",
       "  'rougeL': 0.4800000000000001,\n",
       "  'llm_score': 1.0},\n",
       " 'average': {'embedding_similarity': 0.8218868353922913,\n",
       "  'rougeL': 0.4968724881256851,\n",
       "  'llm_score': 0.9}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_score(rubric, rubric2, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cdcad7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_models(model_rubric_sets, ref_rubrics, questions):\n",
    "    \"\"\"\n",
    "    model_rubric_sets: dict like {\n",
    "        'GPT': [gen_rubric1, gen_rubric2, ...],\n",
    "        'Gemini': [...],\n",
    "        'Mistral': [...]\n",
    "    }\n",
    "    ref_rubrics: list of reference rubrics (same length as each model's list)\n",
    "    questions: list of question strings (same length as ref_rubrics)\n",
    "    \"\"\"\n",
    "\n",
    "    all_results = {}  # model -> list of per-item results\n",
    "\n",
    "    for model_name, gen_rubrics in model_rubric_sets.items():\n",
    "        model_scores = []\n",
    "        print(f\"Evaluating model: {model_name}\")\n",
    "\n",
    "        for ref_rubric, gen_rubric, question in tqdm(zip(ref_rubrics, gen_rubrics, questions)):\n",
    "            res = compute_score(gen_rubric, ref_rubric, question)\n",
    "            model_scores.append(res)\n",
    "\n",
    "        all_results[model_name] = model_scores\n",
    "\n",
    "    # Aggregate results\n",
    "    return summarize_results(all_results)\n",
    "\n",
    "def summarize_results(all_results):\n",
    "    \"\"\"\n",
    "    Summarize results into two tables:\n",
    "      - Table 1: Model-level aggregate metrics\n",
    "      - Table 2: Per-rubric-level metrics\n",
    "    \"\"\"\n",
    "    table1_data = []\n",
    "    table2_data = []\n",
    "\n",
    "    # Collect all unique rubric levels (e.g., \"comprehensive\", \"competent\", etc.)\n",
    "    all_levels = set()\n",
    "    for model_scores in all_results.values():\n",
    "        for item in model_scores:\n",
    "            all_levels.update([k for k in item.keys() if k != \"average\"])\n",
    "    all_levels = sorted(list(all_levels))\n",
    "\n",
    "    for model_name, model_scores in all_results.items():\n",
    "        # --- Table 1 (aggregate) ---\n",
    "        emb_scores, rouge_scores, llm_scores = [], [], []\n",
    "\n",
    "        # --- Table 2 (per-level) ---\n",
    "        level_scores = {lvl: [] for lvl in all_levels}\n",
    "\n",
    "        for res in model_scores:\n",
    "            avg = res.get(\"average\", {})\n",
    "            if avg:\n",
    "                emb_scores.append(avg.get(\"embedding_similarity\", 0))\n",
    "                rouge_scores.append(avg.get(\"rougeL\", 0))\n",
    "                llm_scores.append(avg.get(\"llm_score\", 0))\n",
    "\n",
    "            # collect per-level\n",
    "            for lvl in all_levels:\n",
    "                if lvl in res:\n",
    "                    lvl_avg = np.mean([\n",
    "                        res[lvl].get(\"embedding_similarity\", 0),\n",
    "                        res[lvl].get(\"rougeL\", 0),\n",
    "                        res[lvl].get(\"llm_score\", 0)\n",
    "                    ])\n",
    "                    level_scores[lvl].append(lvl_avg)\n",
    "\n",
    "        # --- Table 1 row ---\n",
    "        mean_emb = np.mean(emb_scores) if emb_scores else 0\n",
    "        mean_rouge = np.mean(rouge_scores) if rouge_scores else 0\n",
    "        mean_llm = np.mean(llm_scores) if llm_scores else 0\n",
    "        overall_mean = np.mean([mean_emb, mean_rouge, mean_llm])\n",
    "        table1_data.append([model_name, mean_emb, mean_rouge, mean_llm, overall_mean])\n",
    "\n",
    "    # --- Convert to DataFrames for nice tabular display ---\n",
    "    table1 = pd.DataFrame(table1_data, columns=[\"Models\", \"Embedding_score\", \"ROUGEL\", \"LLM_score\", \"Mean\"])\n",
    "    print(\"\\nðŸ“Š Table 1: Overall Model Performance\")\n",
    "    print(table1.round(3).to_string(index=False))\n",
    "\n",
    "    return table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6dd95d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import time\n",
    "\n",
    "def load_data_from_csv(file_path):\n",
    "    with open(file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        data = [row for row in csv_reader]\n",
    "        for entry in data:\n",
    "            rubric_levels = ['Comprehensive response', 'Competent response',\n",
    "                            'Partial response', 'Limited response']\n",
    "            rubric = {level: entry[level] for level in rubric_levels if level in entry}\n",
    "            entry['rubric'] = rubric\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions   = [ref['question'] for ref in load_data_from_csv('dataset/gold_samples.csv')]\n",
    "ref_rubrics = [ref['rubric'] for ref in load_data_from_csv('dataset/gold_samples.csv')]\n",
    "gpt4_rubrics = [gen['rubric'] for gen in load_data_from_csv('dataset/gpt4_gen_samples.csv')]\n",
    "mistral_rubrics = [gen['rubric'] for gen in load_data_from_csv('dataset/mistral_gen_samples.csv')]\n",
    "gemini_rubrics = [gen['rubric'] for gen in load_data_from_csv('dataset/gemini_gen_samples.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a5afd96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: GPT4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [06:01, 30.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: Mistral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [06:30, 32.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: Gemini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [06:50, 34.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Table 1: Overall Model Performance\n",
      " Models  Embedding_score  ROUGEL  LLM_score  Mean\n",
      "   GPT4            0.622   0.206      0.779 0.536\n",
      "Mistral            0.612   0.158      0.733 0.501\n",
      " Gemini            0.624   0.213      0.767 0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_rubric_sets = {\n",
    "    \"GPT4\": gpt4_rubrics,\n",
    "    \"Mistral\": mistral_rubrics,\n",
    "    \"Gemini\": gemini_rubrics\n",
    "}\n",
    "\n",
    "table1 = evaluate_models(model_rubric_sets, ref_rubrics, questions)\n",
    "import pickle\n",
    "pickle.dump(table1, open('evaluation_table1.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
